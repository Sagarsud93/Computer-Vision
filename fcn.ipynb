{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fcn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNqUOnusscbeCBlMkWCeAFy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sagarsud93/Computer-Vision_Object_Proposals_OpenCV/blob/master/fcn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "np19E0t-phbi",
        "outputId": "79ed951a-0789-43c6-e462-cfdc52d08847",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "import sys\n",
        "import os\n",
        "\n",
        "from glob import glob\n",
        "import re\n",
        "\n",
        "\n",
        "import cv2\n",
        "import json\n",
        "\n",
        "from skimage import io, transform\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(1234)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGlNZBKP0LAw",
        "outputId": "bded7a1d-3ce3-4538-cc0f-85f44e66a186",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!ls \"/content/drive/My Drive/data_road\"\n",
        "!cd \"/content/drive/My Drive/data_road\""
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "testing  training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2r9WGMwNOgB"
      },
      "source": [
        "def normalize(img, mean, std):\n",
        "    img = img/255.0\n",
        "    img[0] = (img[0] - mean[0]) / std[0]\n",
        "    img[1] = (img[1] - mean[1]) / std[1]\n",
        "    img[2] = (img[2] - mean[2]) / std[2]\n",
        "    img = np.clip(img, 0.0, 1.0)\n",
        "\n",
        "    return img\n",
        "\n",
        "def denormalize(img, mean, std):\n",
        "    img[0] = (img[0] * std[0]) + mean[0]\n",
        "    img[1] = (img[1] * std[1]) + mean[1]\n",
        "    img[2] = (img[2] * std[2]) + mean[2]\n",
        "    img = img * 255\n",
        "\n",
        "    img = np.clip(img, 0, 255)\n",
        "    return img\n",
        "\n",
        "\n",
        "def get_label_paths(label_path):\n",
        "    label_paths = {re.sub(r'_(lane|road)_', '_', os.path.basename(path)): path\n",
        "                   for path in glob(os.path.join(label_path, '*_road_*.png'))}\n",
        "\n",
        "    return label_paths\n",
        "\n",
        "def get_test_paths(test_path):\n",
        "    test_paths = [os.path.basename(path)\n",
        "                      for path in glob(os.path.join(test_path, '*.png'))]\n",
        "\n",
        "    return test_paths\n",
        "\n",
        "def make_layers(cfg, batch_norm=False):\n",
        "    layers = []\n",
        "    in_channels = 3\n",
        "    for v in cfg:\n",
        "        if v == 'M':\n",
        "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "        else:\n",
        "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
        "            if batch_norm:\n",
        "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
        "            else:\n",
        "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
        "            in_channels = v\n",
        "\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "def gen_test_output(n_class, testloader, model, test_folder):\n",
        "    model.eval();\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(testloader):\n",
        "            sample = data\n",
        "            images = sample['image']\n",
        "            images = images.float()\n",
        "            images = Variable(images.to(device))\n",
        "\n",
        "            output = model(images)\n",
        "            output = torch.sigmoid(output)\n",
        "            output = output.cpu()\n",
        "            N, c, h, w = output.shape\n",
        "            pred = np.squeeze(output.detach().cpu().numpy(), axis=0)\n",
        "\n",
        "            pred = pred.transpose((1, 2, 0))\n",
        "            pred = pred.argmax(axis=2)\n",
        "            pred = (pred > 0.5)\n",
        "\n",
        "            pred = pred.reshape(*pred.shape, 1)\n",
        "            pred = np.concatenate((pred, np.invert(pred)), axis=2).astype('float')\n",
        "            pred = np.concatenate((pred, np.zeros((*pred[:,:,0].shape, 1))), axis=2).astype('float')\n",
        "\n",
        "            pred[pred == 1.0] = 127.0\n",
        "            # images = images.cpu().detach().numpy()\n",
        "            # images = np.squeeze(images)\n",
        "            # images = images.transpose((1, 2, 0))\n",
        "\n",
        "            # images = denormalize(images, mean=[0.485, 0.456, 0.406],\n",
        "            #                         std=[0.229, 0.224, 0.225])\n",
        "            test_paths = get_test_paths(test_folder)\n",
        "            output = resize_label(os.path.join(test_folder, test_paths[i]), pred)\n",
        "            output = output/127.0\n",
        "            output = np.clip(output, 0.0, 1.0)\n",
        "            yield test_paths[i], output\n",
        "\n",
        "def save_inference_samples(output_dir, testloader, model, test_folder):\n",
        "    print('Training Finished. Saving test images to: {}'.format(output_dir))\n",
        "    image_outputs = gen_test_output(2, testloader, model, test_folder)\n",
        "    for name, image in image_outputs:\n",
        "        plt.imsave(os.path.join(output_dir, name), image)\n",
        "\n",
        "def resize_label(image_path, label):\n",
        "    image = io.imread(image_path)\n",
        "    label = transform.resize(label, image.shape)\n",
        "    output = cv2.addWeighted(image, 0.6, label, 0.4, 0, dtype = 0)\n",
        "    return output"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcxV4CfOKXSH"
      },
      "source": [
        "\n",
        "class KittiDatasetTrain(Dataset):\n",
        "    def __init__(self, rootdir, transform=None):\n",
        "        self.transform = transform\n",
        "        self.rootdir = rootdir\n",
        "        self.traindir = rootdir + \"/training/image_2\"\n",
        "        self.labeldir = rootdir + \"/training/gt_image_2\"\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        label_paths = get_label_paths(label_path=self.labeldir)\n",
        "        image_path = list(label_paths)[index]\n",
        "        img = io.imread(os.path.join(self.traindir, image_path))\n",
        "        label = io.imread(label_paths[image_path])\n",
        "        background_color = np.array([255, 0, 0])\n",
        "        img = cv2.resize(img, (256, 256))\n",
        "        label = cv2.resize(label, (256, 256))\n",
        "        gt_bg = np.all(label == background_color, axis=2)\n",
        "        gt_bg = gt_bg.reshape(*gt_bg.shape, 1)\n",
        "        gt_image = np.concatenate((gt_bg, np.invert(gt_bg)), axis=2)\n",
        "        img = normalize(img, mean=[0.485, 0.456, 0.406],\n",
        "                                std=[0.229, 0.224, 0.225])\n",
        "\n",
        "        img = img.transpose((2, 0, 1))\n",
        "        gt_image = gt_image.transpose((2, 0, 1))\n",
        "        gt_image = gt_image.astype(\"float\")\n",
        "\n",
        "        img = torch.from_numpy(img)\n",
        "        #img = torch.clamp(img, 0, 1)\n",
        "        sample = {'image': img,\n",
        "                'label': torch.from_numpy(gt_image)}\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample\n",
        "\n",
        "    def __len__(self):\n",
        "        path, dirs, files = next(os.walk(self.traindir))\n",
        "        n = len(files)\n",
        "        return n # of how many examples(images?)\n",
        "\n",
        "class KittiDatasetTest(Dataset):\n",
        "    def __init__(self, rootdir, transform=None):\n",
        "        self.transform = transform\n",
        "        self.rootdir = rootdir\n",
        "        self.testdir = rootdir + \"/testing/image_2\"\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        test_paths = get_test_paths(test_path=self.testdir)\n",
        "        image_path = test_paths[index]\n",
        "        img = io.imread(os.path.join(self.testdir, image_path))\n",
        "\n",
        "        img = cv2.resize(img, (256, 256))\n",
        "        img = normalize(img, mean=[0.485, 0.456, 0.406],\n",
        "                                std=[0.229, 0.224, 0.225])\n",
        "        img = img.transpose((2, 0, 1))\n",
        "\n",
        "        img = torch.from_numpy(img)\n",
        "        #img = torch.clamp(img, 0, 1)\n",
        "        sample = {'image': img}\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample\n",
        "\n",
        "    def __len__(self):\n",
        "        path, dirs, files = next(os.walk(self.testdir))\n",
        "        n = len(files)\n",
        "        return n"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8g2rFfyO3i-"
      },
      "source": [
        "import torchvision.models as models\n",
        "#from utils import make_layers\n",
        "\n",
        "ranges = {\n",
        "    'vgg11': ((0, 3), (3, 6),  (6, 11),  (11, 16), (16, 21)),\n",
        "    'vgg13': ((0, 5), (5, 10), (10, 15), (15, 20), (20, 25)),\n",
        "    'vgg16': ((0, 5), (5, 10), (10, 17), (17, 24), (24, 31)),\n",
        "    'vgg19': ((0, 5), (5, 10), (10, 19), (19, 28), (28, 37))\n",
        "}\n",
        "\n",
        "cfg = {\n",
        "    'vgg11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'vgg13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'vgg16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "    'vgg19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
        "}\n",
        "\n",
        "class VGGNet(models.VGG):\n",
        "    def __init__(self, pretrained=True, model='vgg16', requires_grad=True, remove_fc=True, show_params=False):\n",
        "        super().__init__(make_layers(cfg[model]))\n",
        "        self.ranges = ranges[model]\n",
        "\n",
        "        if pretrained:\n",
        "            exec(\"self.load_state_dict(models.%s(pretrained=True).state_dict())\" % model)\n",
        "\n",
        "        if not requires_grad:\n",
        "            for param in super().parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "        if remove_fc:  # delete redundant fully-connected layer params, can save memory\n",
        "            del self.classifier\n",
        "\n",
        "        if show_params:\n",
        "            for name, param in self.named_parameters():\n",
        "                print(name, param.size())\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = {}\n",
        "\n",
        "        # get the output of each maxpooling layer (5 maxpool in VGG net)\n",
        "        for idx in range(len(self.ranges)):\n",
        "            for layer in range(self.ranges[idx][0], self.ranges[idx][1]):\n",
        "                x = self.features[layer](x)\n",
        "            output[\"x%d\"%(idx+1)] = x\n",
        "\n",
        "        return output"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GypJixvwPPdk"
      },
      "source": [
        "class FCN32s(nn.Module):\n",
        "\n",
        "    def __init__(self, pretrained_net, n_class):\n",
        "        super().__init__()\n",
        "        self.n_class = n_class\n",
        "        self.pretrained_net = pretrained_net\n",
        "        self.relu    = nn.ReLU(inplace=True)\n",
        "        self.deconv1 = nn.ConvTranspose2d(512, 512, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
        "        self.bn1     = nn.BatchNorm2d(512)\n",
        "        self.deconv2 = nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
        "        self.bn2     = nn.BatchNorm2d(256)\n",
        "        self.deconv3 = nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
        "        self.bn3     = nn.BatchNorm2d(128)\n",
        "        self.deconv4 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
        "        self.bn4     = nn.BatchNorm2d(64)\n",
        "        self.deconv5 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
        "        self.bn5     = nn.BatchNorm2d(32)\n",
        "        self.classifier = nn.Conv2d(32, n_class, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.pretrained_net(x)\n",
        "        x5 = output['x5']  # size=(N, 512, x.H/32, x.W/32)\n",
        "\n",
        "        score = self.bn1(self.relu(self.deconv1(x5)))     # size=(N, 512, x.H/16, x.W/16)\n",
        "        score = self.bn2(self.relu(self.deconv2(score)))  # size=(N, 256, x.H/8, x.W/8)\n",
        "        score = self.bn3(self.relu(self.deconv3(score)))  # size=(N, 128, x.H/4, x.W/4)\n",
        "        score = self.bn4(self.relu(self.deconv4(score)))  # size=(N, 64, x.H/2, x.W/2)\n",
        "        score = self.bn5(self.relu(self.deconv5(score)))  # size=(N, 32, x.H, x.W)\n",
        "        score = self.classifier(score)                    # size=(N, n_class, x.H/1, x.W/1)\n",
        "\n",
        "        return score  # size=(N, n_class, x.H/1, x.W/1)\n",
        "\n",
        "\n",
        "class FCN16s(nn.Module):\n",
        "\n",
        "    def __init__(self, pretrained_net, n_class):\n",
        "        super().__init__()\n",
        "        self.n_class = n_class\n",
        "        self.pretrained_net = pretrained_net\n",
        "        self.relu    = nn.ReLU(inplace=True)\n",
        "        self.deconv1 = nn.ConvTranspose2d(512, 512, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
        "        self.bn1     = nn.BatchNorm2d(512)\n",
        "        self.deconv2 = nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
        "        self.bn2     = nn.BatchNorm2d(256)\n",
        "        self.deconv3 = nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
        "        self.bn3     = nn.BatchNorm2d(128)\n",
        "        self.deconv4 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
        "        self.bn4     = nn.BatchNorm2d(64)\n",
        "        self.deconv5 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
        "        self.bn5     = nn.BatchNorm2d(32)\n",
        "        self.classifier = nn.Conv2d(32, n_class, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.pretrained_net(x)\n",
        "        x5 = output['x5']  # size=(N, 512, x.H/32, x.W/32)\n",
        "        x4 = output['x4']  # size=(N, 512, x.H/16, x.W/16)\n",
        "\n",
        "        score = self.relu(self.deconv1(x5))               # size=(N, 512, x.H/16, x.W/16)\n",
        "        score = self.bn1(score + x4)                      # element-wise add, size=(N, 512, x.H/16, x.W/16)\n",
        "        score = self.bn2(self.relu(self.deconv2(score)))  # size=(N, 256, x.H/8, x.W/8)\n",
        "        score = self.bn3(self.relu(self.deconv3(score)))  # size=(N, 128, x.H/4, x.W/4)\n",
        "        score = self.bn4(self.relu(self.deconv4(score)))  # size=(N, 64, x.H/2, x.W/2)\n",
        "        score = self.bn5(self.relu(self.deconv5(score)))  # size=(N, 32, x.H, x.W)\n",
        "        score = self.classifier(score)                    # size=(N, n_class, x.H/1, x.W/1)\n",
        "\n",
        "        return score  # size=(N, n_class, x.H/1, x.W/1)\n",
        "\n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tf3j05PsRLQG",
        "outputId": "086c570e-35ab-4370-9c29-f9698d1332d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import torch\n",
        "import cv2\n",
        "from glob import glob\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import argparse\n",
        "import os\n",
        "#from torch.utils.data import DataLoader\n",
        "#from dataset import KittiDatasetTrain, KittiDatasetTest\n",
        "#from vgg import VGGNet\n",
        "#from fcn import FCN8s\n",
        "#from utils import save_inference_samples, get_test_paths\n",
        "\n",
        "#np.random.seed(1234)\n",
        "\n",
        "##parser = argparse.ArgumentParser()\n",
        "\n",
        "#parser.add_argument('--output_dir', type=str, required=True,help='output directory for test inference')\n",
        "output_dir = \"/content/drive/My Drive/data_road/inference\"\n",
        "#parser.add_argument('--root_dir', type=str, required=True,help='root directory for the dataset')\n",
        "root_dir = \"/content/drive/My Drive/data_road\"\n",
        "#parser.add_argument('--model', type=str, default='vgg19',help='model architecture to be used for FCN')\n",
        "model='vgg16'\n",
        "epochs=100\n",
        "n_class=2\n",
        "batch_size=16\n",
        "lr=1e-3\n",
        "momentum=0.9\n",
        "weight_decay=5e-4\n",
        "#parser.add_argument('--epochs', type=int, default=100,help='num of training epochs')\n",
        "#parser.add_argument('--n_class', type=int, default=2,help='number of label classes')\n",
        "#parser.add_argument('--batch_size', type=int, default=16,help='training batch size')\n",
        "#parser.add_argument('--lr', type=float, default=1e-3,help='learning rate')\n",
        "#parser.add_argument('--momentum', type=float, default=0.9,help='momentum for SGD')\n",
        "#parser.add_argument('--weight_decay', type=float, default=5e-4,help='weight decay for L2 penalty')\n",
        "#args = parser.parse_args()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def train(n_epoch, trainloader):\n",
        "    vgg_model = VGGNet('vgg16', requires_grad=True)\n",
        "    model = FCN16s(pretrained_net=vgg_model, n_class=2)\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr,\n",
        "                                momentum, weight_decay)\n",
        "    criterion = nn.BCELoss()\n",
        "\n",
        "    for epoch in range(n_epoch):\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader):\n",
        "            sample = data\n",
        "            images = sample['image']\n",
        "            images = images.float()\n",
        "            labels = sample['label']\n",
        "            labels = labels.float()\n",
        "            images = Variable(images.cuda())\n",
        "            labels = Variable(labels.cuda(), requires_grad=False)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(images)\n",
        "            output = torch.sigmoid(output)\n",
        "            loss = criterion(output, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            if i % 10 == 9:    # print every 10 mini-batches\n",
        "                print('Epoch: %d, Loss: %.4f' %\n",
        "                      (epoch + 1, running_loss / 10))\n",
        "                running_loss = 0.0\n",
        "    return model\n",
        "\n",
        "def main():\n",
        "    kitti_train = KittiDatasetTrain(root_dir)\n",
        "    kitti_test = KittiDatasetTest(root_dir)\n",
        "\n",
        "    trainloader = DataLoader(kitti_train, batch_size)\n",
        "    testloader = DataLoader(kitti_test, batch_size=1)\n",
        "\n",
        "    print(\"Training model..\")\n",
        "    model = train(epochs, trainloader)\n",
        "    print(\"Completed training!\")\n",
        "    print(\"Starting inference...\")\n",
        "    test_folder = os.path.join(root_dir, \"testing/image_2\")\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "    save_inference_samples(output_dir, testloader,\n",
        "                            model, test_folder)\n",
        "    print(\"Inference completed!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training model..\n",
            "Epoch: 1, Loss: 0.7403\n",
            "Epoch: 2, Loss: 0.7291\n",
            "Epoch: 3, Loss: 0.7152\n",
            "Epoch: 4, Loss: 0.7020\n",
            "Epoch: 5, Loss: 0.6895\n",
            "Epoch: 6, Loss: 0.6779\n",
            "Epoch: 7, Loss: 0.6670\n",
            "Epoch: 8, Loss: 0.6567\n",
            "Epoch: 9, Loss: 0.6469\n",
            "Epoch: 10, Loss: 0.6377\n",
            "Epoch: 11, Loss: 0.6288\n",
            "Epoch: 12, Loss: 0.6203\n",
            "Epoch: 13, Loss: 0.6122\n",
            "Epoch: 14, Loss: 0.6043\n",
            "Epoch: 15, Loss: 0.5968\n",
            "Epoch: 16, Loss: 0.5895\n",
            "Epoch: 17, Loss: 0.5824\n",
            "Epoch: 18, Loss: 0.5755\n",
            "Epoch: 19, Loss: 0.5688\n",
            "Epoch: 20, Loss: 0.5622\n",
            "Epoch: 21, Loss: 0.5558\n",
            "Epoch: 22, Loss: 0.5495\n",
            "Epoch: 23, Loss: 0.5432\n",
            "Epoch: 24, Loss: 0.5370\n",
            "Epoch: 25, Loss: 0.5309\n",
            "Epoch: 26, Loss: 0.5246\n",
            "Epoch: 27, Loss: 0.5183\n",
            "Epoch: 28, Loss: 0.5118\n",
            "Epoch: 29, Loss: 0.5051\n",
            "Epoch: 30, Loss: 0.4980\n",
            "Epoch: 31, Loss: 0.4906\n",
            "Epoch: 32, Loss: 0.4826\n",
            "Epoch: 33, Loss: 0.4739\n",
            "Epoch: 34, Loss: 0.4646\n",
            "Epoch: 35, Loss: 0.4544\n",
            "Epoch: 36, Loss: 0.4433\n",
            "Epoch: 37, Loss: 0.4316\n",
            "Epoch: 38, Loss: 0.4193\n",
            "Epoch: 39, Loss: 0.4070\n",
            "Epoch: 40, Loss: 0.3947\n",
            "Epoch: 41, Loss: 0.3827\n",
            "Epoch: 42, Loss: 0.3709\n",
            "Epoch: 43, Loss: 0.3594\n",
            "Epoch: 44, Loss: 0.3482\n",
            "Epoch: 45, Loss: 0.3371\n",
            "Epoch: 46, Loss: 0.3261\n",
            "Epoch: 47, Loss: 0.3153\n",
            "Epoch: 48, Loss: 0.3047\n",
            "Epoch: 49, Loss: 0.2944\n",
            "Epoch: 50, Loss: 0.2842\n",
            "Epoch: 51, Loss: 0.2745\n",
            "Epoch: 52, Loss: 0.2650\n",
            "Epoch: 53, Loss: 0.2560\n",
            "Epoch: 54, Loss: 0.2475\n",
            "Epoch: 55, Loss: 0.2394\n",
            "Epoch: 56, Loss: 0.2317\n",
            "Epoch: 57, Loss: 0.2245\n",
            "Epoch: 58, Loss: 0.2176\n",
            "Epoch: 59, Loss: 0.2111\n",
            "Epoch: 60, Loss: 0.2051\n",
            "Epoch: 61, Loss: 0.1993\n",
            "Epoch: 62, Loss: 0.1939\n",
            "Epoch: 63, Loss: 0.1888\n",
            "Epoch: 64, Loss: 0.1839\n",
            "Epoch: 65, Loss: 0.1793\n",
            "Epoch: 66, Loss: 0.1750\n",
            "Epoch: 67, Loss: 0.1709\n",
            "Epoch: 68, Loss: 0.1670\n",
            "Epoch: 69, Loss: 0.1633\n",
            "Epoch: 70, Loss: 0.1598\n",
            "Epoch: 71, Loss: 0.1564\n",
            "Epoch: 72, Loss: 0.1532\n",
            "Epoch: 73, Loss: 0.1501\n",
            "Epoch: 74, Loss: 0.1472\n",
            "Epoch: 75, Loss: 0.1444\n",
            "Epoch: 76, Loss: 0.1417\n",
            "Epoch: 77, Loss: 0.1391\n",
            "Epoch: 78, Loss: 0.1366\n",
            "Epoch: 79, Loss: 0.1342\n",
            "Epoch: 80, Loss: 0.1319\n",
            "Epoch: 81, Loss: 0.1297\n",
            "Epoch: 82, Loss: 0.1276\n",
            "Epoch: 83, Loss: 0.1255\n",
            "Epoch: 84, Loss: 0.1236\n",
            "Epoch: 85, Loss: 0.1217\n",
            "Epoch: 86, Loss: 0.1198\n",
            "Epoch: 87, Loss: 0.1181\n",
            "Epoch: 88, Loss: 0.1164\n",
            "Epoch: 89, Loss: 0.1147\n",
            "Epoch: 90, Loss: 0.1131\n",
            "Epoch: 91, Loss: 0.1115\n",
            "Epoch: 92, Loss: 0.1100\n",
            "Epoch: 93, Loss: 0.1085\n",
            "Epoch: 94, Loss: 0.1071\n",
            "Epoch: 95, Loss: 0.1057\n",
            "Epoch: 96, Loss: 0.1044\n",
            "Epoch: 97, Loss: 0.1031\n",
            "Epoch: 98, Loss: 0.1018\n",
            "Epoch: 99, Loss: 0.1005\n",
            "Epoch: 100, Loss: 0.0993\n",
            "Completed training!\n",
            "Starting inference...\n",
            "Training Finished. Saving test images to: /content/drive/My Drive/data_road/inference\n",
            "Inference completed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ui5mKGJfA8If"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}